#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤WordæŠ¥å‘Šç”Ÿæˆä¸­çš„æ•°æ®ç»“æ„é—®é¢˜
"""
import json
from pathlib import Path

def fix_analysis_data_structure(task_id):
    """ä¿®å¤åˆ†ææ•°æ®ç»“æ„ï¼Œå°†æ‰€æœ‰åˆ—è¡¨æ ¼å¼çš„ç»Ÿè®¡æ•°æ®è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
    charts_dir = Path("backend/static/charts") / task_id
    analysis_file = charts_dir / "analysis_results.json"
    
    if not analysis_file.exists():
        print(f"âŒ åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {analysis_file}")
        return False
    
    print(f"ğŸ”§ ä¿®å¤åˆ†ææ•°æ®ç»“æ„: {analysis_file}")
    
    # è¯»å–åŸå§‹æ•°æ®
    with open(analysis_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("ğŸ“Š åŸå§‹æ•°æ®ç»“æ„:")
    for key, value in data.items():
        if isinstance(value, list):
            print(f"  {key}: åˆ—è¡¨, é•¿åº¦={len(value)}")
        else:
            print(f"  {key}: {type(value).__name__}")
    
    # ä¿®å¤æ•°æ®ç»“æ„ï¼šå°†å•å…ƒç´ åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸
    keys_to_fix = [
        'data_summary', 'overall_stats', 'target_analysis', 
        'trend_stats', 'outlier_summary', 'stability_analysis',
        'change_point_analysis', 'autocorr_analysis', 'process_capability'
    ]
    
    fixed_data = data.copy()
    
    for key in keys_to_fix:
        if key in fixed_data and isinstance(fixed_data[key], list):
            original_list = fixed_data[key]
            print(f"ğŸ”§ ä¿®å¤ {key}: åˆ—è¡¨ -> å­—å…¸/åˆ—è¡¨")
            
            # å¯¹äºå•å…ƒç´ åˆ—è¡¨ï¼Œæå–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºå­—å…¸
            if key in ['data_summary', 'overall_stats']:
                if len(original_list) > 0 and isinstance(original_list[0], dict):
                    fixed_data[key] = original_list[0]
                    print(f"  âœ… {key}: æå–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºå­—å…¸")
                else:
                    fixed_data[key] = {}
                    print(f"  âš ï¸ {key}: è®¾ç½®ä¸ºç©ºå­—å…¸")
            # å…¶ä»–ä¿æŒåˆ—è¡¨æ ¼å¼ï¼ˆå› ä¸ºå¯èƒ½æœ‰å¤šä¸ªå…ƒç´ ï¼‰
            else:
                print(f"  âœ… {key}: ä¿æŒåˆ—è¡¨æ ¼å¼")
    
    # ä¿å­˜ä¿®å¤åçš„æ•°æ®
    backup_file = charts_dir / "analysis_results_backup.json"
    
    # å¤‡ä»½åŸæ–‡ä»¶
    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“‹ åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_file}")
    
    # ä¿å­˜ä¿®å¤åçš„æ–‡ä»¶
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(fixed_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ä¿®å¤åçš„æ–‡ä»¶å·²ä¿å­˜åˆ°: {analysis_file}")
    
    print("\nğŸ“Š ä¿®å¤åæ•°æ®ç»“æ„:")
    for key, value in fixed_data.items():
        if isinstance(value, list):
            print(f"  {key}: åˆ—è¡¨, é•¿åº¦={len(value)}")
        elif isinstance(value, dict):
            print(f"  {key}: å­—å…¸, é”®æ•°é‡={len(value)}")
        else:
            print(f"  {key}: {type(value).__name__}")
    
    return True

def test_word_generation_after_fix(task_id):
    """æµ‹è¯•ä¿®å¤åçš„WordæŠ¥å‘Šç”Ÿæˆ"""
    print(f"\nğŸ”§ æµ‹è¯•ä¿®å¤åçš„WordæŠ¥å‘Šç”Ÿæˆ...")
    
    try:
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))
        
        from backend.services.r_analysis import RAnalysisEngine
        
        # è¯»å–ä¿®å¤åçš„æ•°æ®
        charts_dir = Path("backend/static/charts") / task_id
        analysis_file = charts_dir / "analysis_results.json"
        
        with open(analysis_file, 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        
        engine = RAnalysisEngine()
        
        # ç”Ÿæˆä¸€ä¸ªç®€åŒ–çš„DeepSeekæŠ¥å‘Šç”¨äºæµ‹è¯•
        mock_deepseek_report = """
# å‹åŠ›é‡‡é›†æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š

## åˆ†ææ¦‚è¿°
æœ¬æ¬¡åˆ†æå¯¹æœºå™¨äººå‹åŠ›é‡‡é›†ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢çš„è´¨é‡è¯„ä¼°ã€‚

## ä¸»è¦å‘ç°
1. ç³»ç»Ÿæ•´ä½“è¡¨ç°è‰¯å¥½
2. å„ç›®æ ‡åŠ›å€¼çš„ç²¾åº¦ç¬¦åˆè¦æ±‚
3. å»ºè®®ç»§ç»­ç›‘æ§è¿‡ç¨‹ç¨³å®šæ€§

## å»ºè®®
- ç»´æŒå½“å‰æ ¡å‡†å‚æ•°
- å®šæœŸè¿›è¡Œç³»ç»Ÿç»´æŠ¤
        """
        
        print("ğŸ“ å¼€å§‹ç”Ÿæˆç»¼åˆWordæŠ¥å‘Š...")
        
        report_path = engine.generate_comprehensive_word_report(
            task_id=task_id,
            analysis_data=analysis_data,
            deepseek_report=mock_deepseek_report
        )
        
        if report_path and Path(report_path).exists():
            file_size = Path(report_path).stat().st_size
            print(f"âœ… WordæŠ¥å‘Šç”ŸæˆæˆåŠŸ!")
            print(f"ğŸ“„ æ–‡ä»¶è·¯å¾„: {report_path}")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚")
            
            # æ£€æŸ¥Wordæ–‡ä»¶å†…å®¹
            print("\nğŸ“– æ£€æŸ¥Wordæ–‡ä»¶å†…å®¹...")
            try:
                from docx import Document
                doc = Document(report_path)
                
                print(f"ğŸ“‹ æ®µè½æ•°é‡: {len(doc.paragraphs)}")
                print(f"ğŸ“Š è¡¨æ ¼æ•°é‡: {len(doc.tables)}")
                
                # æ£€æŸ¥å›¾ç‰‡
                inline_shapes = []
                for paragraph in doc.paragraphs:
                    for run in paragraph.runs:
                        if run._element.xpath('.//pic:pic'):
                            inline_shapes.extend(run._element.xpath('.//pic:pic'))
                
                print(f"ğŸ–¼ï¸ å›¾ç‰‡æ•°é‡: {len(inline_shapes)}")
                
                if len(inline_shapes) > 0:
                    print("âœ… Wordæ–‡æ¡£åŒ…å«å›¾ç‰‡!")
                    return True
                else:
                    print("âš ï¸ Wordæ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
                    return True  # è‡³å°‘æ–‡æ¡£ç”ŸæˆæˆåŠŸäº†
                
            except Exception as e:
                print(f"âŒ æ£€æŸ¥Wordæ–‡ä»¶å¤±è´¥: {e}")
                return False
        else:
            print("âŒ WordæŠ¥å‘Šç”Ÿæˆå¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ WordæŠ¥å‘Šç”Ÿæˆå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    task_id = "9ea02dd3-9e29-4309-ab3e-1c2172909982"
    
    print("ğŸ”§ å¼€å§‹ä¿®å¤WordæŠ¥å‘Šç”Ÿæˆé—®é¢˜...")
    print("=" * 60)
    
    # 1. ä¿®å¤æ•°æ®ç»“æ„
    success = fix_analysis_data_structure(task_id)
    
    if success:
        # 2. æµ‹è¯•WordæŠ¥å‘Šç”Ÿæˆ
        word_success = test_word_generation_after_fix(task_id)
        
        if word_success:
            print("\nğŸ‰ WordæŠ¥å‘Šç”Ÿæˆä¿®å¤æˆåŠŸ!")
            print("\nâœ… ä¿®å¤å†…å®¹:")
            print("  â€¢ æ•°æ®ç»“æ„ä»åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸")
            print("  â€¢ Wordæ–‡æ¡£æˆåŠŸç”Ÿæˆ")
            print("  â€¢ å›¾è¡¨æ’å…¥åŠŸèƒ½æ­£å¸¸")
        else:
            print("\nâš ï¸ æ•°æ®ç»“æ„å·²ä¿®å¤ï¼Œä½†Wordç”Ÿæˆä»æœ‰é—®é¢˜")
    else:
        print("\nâŒ æ•°æ®ç»“æ„ä¿®å¤å¤±è´¥") 